# text-to-image-stable-diffusion
# ğŸ¨ Text-to-Image Generation using Stable Diffusion XL (SDXL)

This project uses **Stable Diffusion XL (SDXL)** models to generate photorealistic images from natural language prompts. It leverages Hugging Faceâ€™s ğŸ¤— **Diffusers** library and PyTorch for high-quality text-to-image generation.

---

## ğŸ—ï¸ System Workflow

1. **Input**:  
   - Natural language text prompts (e.g., *â€œA cinematic 35mm film still of a rainy street at nightâ€*)  
   - Optional negative prompts to filter out unwanted features.

2. **Preprocessing**:
   - Tokenization of text using `CLIPTokenizer`.
   - Encoding prompts into latent space for SDXL.

3. **Model**:
   - **Stable Diffusion XL** (models like `stabilityai/sdxl-turbo`).
   - Components: UNet, VAE (AutoencoderKL), CLIPTextModel.

4. **Image Generation**:
   - Uses denoising diffusion probabilistic models.
   - Parameters: `num_inference_steps`, `guidance_scale`, random seeds.

5. **Output**:  
   - High-resolution images (PNG/JPG format) saved locally or displayed in notebooks.

---

## ğŸŒŸ Why Stable Diffusion XL?

âœ… Photorealistic results  
âœ… Prompt engineering support (positive & negative)  
âœ… Optimized for CUDA GPUs  
âœ… Beginner-friendly, modular codebase  

---

## ğŸ”¥ Example

**Prompt:**  
> *â€œA Girl holding a card saying do you love me to a guy, real-life style, high quality, detailed and perfect face.â€*

**Negative Prompt:**  
> *â€œlow quality, bad anatomy, deformed, blurry, ugly, noise, bad hands, extra limbsâ€*

ğŸ“· *Result saved as `examples/sample_output.png`*

---

## ğŸ“¦ Repository Structure

